{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 implement a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, wordlist):\n",
    "        self.wordlist = wordlist\n",
    "\n",
    "    def count_labels(self, data):\n",
    "        \"\"\"\n",
    "        Count the number of positive labels and negative labels.\n",
    "        Returns (a tuple or a numpy array of two elements):\n",
    "            * negative_count: a non-negative integer, which represents the number of negative labels (non-spam emails);\n",
    "            * positive_count: a non-negative integer, which represents the number of positive labels (spam emails).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        positive_count = sum([y for y, x in data])\n",
    "        negative_count = len(data) - positive_count\n",
    "        return np.array([negative_count, positive_count])\n",
    "\n",
    "    def count_words(self, wordlist, data):\n",
    "        \"\"\"\n",
    "        Count the number of times that each word appears in emails under a given label.\n",
    "        Returns (a numpy array):\n",
    "            * word_counts: a numpy array with shape (2, L), where L is the length of $wordlist,\n",
    "                - word_counts[0, i] represents the number of times that word $wordlist[i] appears in non-spam (negative) emails, and\n",
    "                - word_counts[1, i] represents the number of times that word $wordlist[i] appears in spam (positive) emails.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        word_vector = np.zeros((2, len(wordlist)))\n",
    "        \n",
    "        for i in range(len(wordlist)):\n",
    "            for j in range(len(data)):\n",
    "                if wordlist[i] in data[j][1]:\n",
    "                    if data[j][0] == 1:\n",
    "                        word_vector[1,i] += 1\n",
    "                    else:\n",
    "                        word_vector[0,i] += 1\n",
    "        return word_vector\n",
    "\n",
    "    def calculate_probability(self, label_counts, word_counts):\n",
    "        \"\"\"\n",
    "        Calculate the probabilities, both the prior and likelihood.\n",
    "        Returns (a pair of numpy array):\n",
    "            * prior_probs: a numpy array with shape (2, ), only two elements, where\n",
    "                - prior_probs[0] is the prior probability of negative labels, and\n",
    "                - prior_probs[1] is the prior probability of positive labels.\n",
    "            * likelihood_probs: a numpy array with shape (2, L), where L is the length of the word list,\n",
    "                - likelihood_probs[0, i] represents the likelihood probability of the $i-th word in the word list, given that the email is non-spam (negative), and\n",
    "                - likelihood_probs[1, i] represents the likelihood probability of the $i-th word in the word list, given that the email is spam (positive).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        prior_probs = np.zeros((2,))\n",
    "        likelihood_probs = np.zeros((2,len(self.wordlist)))\n",
    "        \n",
    "        negative_probs = label_counts[0]/ (label_counts[0] + label_counts[1])\n",
    "        positive_probs = 1 - negative_probs\n",
    "        \n",
    "        prior_probs[0] = negative_probs\n",
    "        prior_probs[1] = positive_probs\n",
    "        \n",
    "        \n",
    "        nonspam_probs = (word_counts[0]+1)/(label_counts[0]+2)\n",
    "        spam_probs = (word_counts[1]+1)/(label_counts[1]+2)\n",
    "        likelihood_probs[0] = nonspam_probs\n",
    "        likelihood_probs[1] = spam_probs\n",
    "        \n",
    "        return (prior_probs, likelihood_probs)\n",
    "\n",
    "    def fit(self, data):\n",
    "        label_counts = self.count_labels(data)\n",
    "        word_counts = self.count_words(self.wordlist, data)\n",
    "\n",
    "        self.prior_probs, self.likelihood_probs = self.calculate_probability(label_counts, word_counts)\n",
    "\n",
    "        # TO AVOID NUMBER OVERFLOW here we use log probability instead.\n",
    "        self.log_prior_probs = np.log(self.prior_probs)\n",
    "        self.log_likelihood_probs = np.dstack([np.log(1 - self.likelihood_probs), np.log(self.likelihood_probs)])\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict whether email $x is a spam or not.\n",
    "        Returns:\n",
    "            * y: a boolean value indicating whether $x is a spam or not.\n",
    "        \"\"\"        \n",
    "        spam_prob = self.log_prior_probs[1]\n",
    "        nonspam_prob = self.log_prior_probs[0]\n",
    "        \n",
    "        for i in range(len(self.wordlist)):\n",
    "            if self.wordlist[i] in x:\n",
    "                exists = 1\n",
    "            else:\n",
    "                exists = 0\n",
    "            \n",
    "            nonspam_prob += self.log_likelihood_probs[0][i][exists]\n",
    "            \n",
    "            spam_prob += self.log_likelihood_probs[1][i][exists]\n",
    "            \n",
    "        if spam_prob > nonspam_prob:\n",
    "            y = 1\n",
    "        else:\n",
    "            y =0\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read the dataset from the file given by name $filename.\n",
    "    The returned object should be a list of pairs of data. In each pair: the first element is 1 (for spam emails) \n",
    "    or 0 (for non-spam emails), the second element is a list of words in the email.\n",
    "    The returned list: \n",
    "        [\n",
    "            (1 , ['a', 'b', 'c']),\n",
    "            (0, ['d', 'e', 'f']),\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return [(int(y), x) for y, *x in [line.strip().split() for line in f]]\n",
    "\n",
    "def split_train(original_train_data, size=4000):\n",
    "    return original_train_data[:size], original_train_data[size:]\n",
    "\n",
    "\n",
    "def create_wordlist(original_train_data, threshold=26):\n",
    "    \"\"\"\n",
    "    Create a word list from the original training set.\n",
    "    Only get a word if it appears in at least $threshold emails.\n",
    "    Returns:\n",
    "        * a python list containing all the words that occur in at least $threshold emails.\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for each_email in original_train_data:\n",
    "        for word in set(each_email[1]):\n",
    "            vocab_dict[word] = vocab_dict.get(word, 0) + 1\n",
    "\n",
    "    wordlist = []\n",
    "    for key, val in vocab_dict.items():\n",
    "        if val >= threshold:\n",
    "            wordlist.append(key)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold to determine whether to include a word in the dictionary/wordlist.\n",
    "# ie. only words with frequency higher than threshold are included\n",
    "THRESHOLD = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of words: 3048\n",
      "Validation error, # =   61, % =   6.1000%.\n"
     ]
    }
   ],
   "source": [
    "original_train_data = read_data('spam_train.txt')\n",
    "\n",
    "# further split the data into a training set and a validation set\n",
    "train_data, val_data = split_train(original_train_data)\n",
    "\n",
    "# Create the word list.\n",
    "wordlist = create_wordlist(original_train_data, 26)\n",
    "print(\"Total # of words:\", len(wordlist))\n",
    "\n",
    "# fit the model using train_data\n",
    "model = Model(wordlist)\n",
    "model.fit(train_data)\n",
    "\n",
    "\n",
    "# TODO\n",
    "# calculate the error rate on val_data (when threshold=26)\n",
    "# print out the error rate\n",
    "error_count = sum([y != model.predict(x) for y, x in val_data])\n",
    "error_percentage = error_count / len(val_data) * 100\n",
    "\n",
    "print(\"Validation error, # = {:>4d}, % = {:>8.4f}%.\".format(error_count, error_percentage)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 try different thresholds, find the optimal threshold (which gives minimum validation error), print out the test error at the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(model, data):\n",
    "    \n",
    "    error_count = sum([y != model.predict(x) for y, x in data])\n",
    "    return 100.0 * error_count / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = list(range(1, 35))\n",
    "train_error = []\n",
    "val_error = []\n",
    "test_error = []\n",
    "original_train_data = read_data('spam_train.txt')\n",
    "train_data, val_data = split_train(original_train_data)\n",
    "test_data = read_data('spam_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With threshold 1....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1055289a0954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# fit model using the wordlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# compute classification error rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a01515261fd6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a01515261fd6>\u001b[0m in \u001b[0;36mcount_words\u001b[0;34m(self, wordlist, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mword_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for th in thresholds:\n",
    "    print('With threshold {}....'.format(th))\n",
    "    time1 = time.time()\n",
    "\n",
    "    # vocabulary selection\n",
    "    wordlist = create_wordlist(original_train_data, th)\n",
    "\n",
    "    # fit model using the wordlist\n",
    "    model = Model(wordlist)\n",
    "    model.fit(train_data)\n",
    "\n",
    "    # compute classification error rates\n",
    "    err_train = compute_error_rate(model, train_data)\n",
    "    err_val = compute_error_rate(model, val_data)\n",
    "    err_test = compute_error_rate(model, test_data)\n",
    "\n",
    "    # store results for plotting\n",
    "    train_error.append(err_train)\n",
    "    val_error.append(err_val)\n",
    "    test_error.append(err_test)\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(\"train:{} val:{} test:{} len(V)={}\".format(err_train, err_val, err_test, len(wordlist)))\n",
    "    print('time: {}'.format(time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation error rate vs. the thresholds\n",
    "# choose the threshold with the minimal validation error rate and report the corresponding test error rate\n",
    "\n",
    "# TODO\n",
    "plt.figure()\n",
    "plt.plot(range(1, 35),train_error,label=\"training error\")\n",
    "plt.plot(range(1, 35),val_error,label=\"validation error\")\n",
    "plt.xlabel('thresholds')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('The training and validation error rate vs. the thresholds')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_value = min(test_error)\n",
    "opt = test_error.index(min_value)\n",
    "print('Best performance at validated threshold {} with test error rate {}.'.format(opt, test_error[opt]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
